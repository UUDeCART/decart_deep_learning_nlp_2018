{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Recurrent Neural Networks\n",
    "\n",
    "### The idea behind RNNs is to make use of sequential information\n",
    "\n",
    "### Neural Networks are called <i>recurrent</i> because they perform the same task for every element of a sequence\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Output depends on previous computations so we say RNNs have memory\n",
    "<img src=\"./images/memory.jpg\" width=\"400px\">\n",
    "<BR>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### The formulas that govern the computation are as follows:\n",
    "#### * $x_t$ is the input at time step t. For example, $x_1$ could be a one-hot vector corresponding to the second word of a sentence.\n",
    "#### * $s_t$ is the hidden state at time step t. It’s the “memory” of the network. $s_t$ is calculated based on the previous hidden state and the input at the current step: $s_t=f(U{x_t} + Ws_{t-1})$. The function f usually is a nonlinearity such as tanh or ReLU.  $s_{-1}$, which is required to calculate the first hidden state, is typically initialized to all zeroes.\n",
    "#### * $o_t$ is the output at step t. For example, if we wanted to predict the next word in a sentence it would be a vector of probabilities across our vocabulary. $o_t = \\mathrm{softmax}(Vs_t)$.\n",
    "<BR>\n",
    "<center><img src=\"./images/rnn1.jpg\" width=\"600px\"></center>\n",
    "<BR>\n",
    "#### Unlike a traditional deep neural network, which uses different parameters at each layer, a RNN shares the same parameters (U, V, W above) across all steps. \n",
    "#### We are performing the same task at each step, just with different inputs! \n",
    "#### This greatly reduces the total number of parameters we need to learn.\n",
    "\n",
    "### Training through Back-propagation with a <i>twist</i>!\n",
    "#### In order to calculate the gradient at t=4 we would need to backpropagate 3 steps and sum up the gradients. This is called Backpropagation Through Time (BPTT). \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### The Problem of Long-Term Dependencies\n",
    "\n",
    "#### Predict the last word in “the clouds are in the <i>sky</i>\"\n",
    "\n",
    "<BR>\n",
    "<center><img src=\"./images/rnn2.png\" width=\"600px\"></center>\n",
    "<BR>\n",
    "   \n",
    "#### Consider predicting the last word in the text “I grew up in <i>France</i>… I speak fluent <i>French</i>.” \n",
    "##### The issue is that we need long memory conext!\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Long Short Term Memory (LSTM) networks\n",
    "#### A special kind of RNN, capable of learning long-term dependencies!\n",
    "<BR>\n",
    "<center><img src=\"./images/rnn3.png\" width=\"800px\"></center>\n",
    "#### <center> Traditional RNN has only 1 neuron</center>\n",
    "<BR>\n",
    "<BR>\n",
    "<center><img src=\"./images/rnn4.png\" width=\"800px\"></center>\n",
    "#### <center> LSTM has 4 neurons !</center>\n",
    "<BR>\n",
    "    \n",
    "### Let's Watch a Video about LSTMs\n",
    "\n",
    "\n",
    "[A Gentle Introduction to LSTMs](https://www.youtube.com/watch?v=WCUNPb-5EYI)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Gates\n",
    " * Gates --> optionally let information through. \n",
    " * Composed of a sigmoid neural net layer and a pointwise multiplication operation.\n",
    " * 0: let nothing through; 1: let everything through\n",
    "<BR>\n",
    "<BR>    \n",
    "<center><img src=\"./images/rnn7.png\" width=\"150px\"></center>\n",
    "<BR>\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three gates protect and control the cell state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Step 1 - Memory Step\n",
    "### Evaluate $h_{t−1}$ and $x_t$ and outputs a number between 0 and 1 for each number in the cell state $C_{t−1}$. \n",
    "### 1 represents “completely keep this” $\\space\\space\\space\\space\\space$ 0 represents “completely get rid of this.”\n",
    "<BR>\n",
    "<center><img src=\"./images/rnn8.png\" width=\"600px\"></center>\n",
    "<BR>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Step 2 - What new information we’re going to store in the cell state\n",
    "### 1) Sigmoid layer called the “input gate layer” decides which values we’ll update. \n",
    "### 2) Tanh layer creates a vector of new candidate values, $\\hat{C}_t$, that could be added to the state. \n",
    "<BR>\n",
    "<center><img src=\"./images/rnn10.png\" width=\"600px\"></center>\n",
    "<BR>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Step 3 - Update old cell state, $C{t−1}$ into new cell state $C_t$.\n",
    "###  The previous steps already decided what to do, we just need to actually do it.\n",
    "### 1) Multiply old state by $f_t$, forgetting things we decided to forget earlier. \n",
    "### 2) Add $i_t ∗ \\hat{C}_t$. This is the new candidate values, scaled by how much we decided to update each state value.\n",
    "<BR>\n",
    "<center><img src=\"./images/rnn11.png\" width=\"600px\"></center>\n",
    "<BR>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Step 4 - Filtered Output Based on Our Cell State\n",
    "### 1) Sigmoid layer decides what parts of cell state to output. \n",
    "### 2) Cell state travels through tanh (pushes values between −1 and 1)\n",
    "### 3) Multiply it by output of the sigmoid gate --> This only outputs the parts we want.\n",
    "<BR>\n",
    "<center><img src=\"./images/rnn12.png\" width=\"600px\"></center>\n",
    "<BR>\n",
    "    \n",
    "### Why are there 2 $h_t$ outputs ???\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Sequence Labeling\n",
    "\n",
    "### Subtask of information extraction - name-entity recoginition\n",
    "### Classify named entities in text into \n",
    "* Names of persons\n",
    "* Organizations\n",
    "* Locations\n",
    "* Expressions of times\n",
    "* Quantities\n",
    "\n",
    "### <U>IOB (Inside-Outside-Beginning) Format</U>\n",
    "#### Example:\n",
    "* I_O complained_O to_O Microsoft_I-ORG about_O Bill_I-PER Gates_I-PER\n",
    "* They_O told_O me_O to_O see_O the_O mayor_O of_O New_I-LOC York_I-LOC\n",
    "<BR>\n",
    "\n",
    "* <B>Advantages:</B> Very fine-grained annotation\n",
    "* <B>Disadvatages:</B> Requires annotation !!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
